{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch gradio -q"
      ],
      "metadata": {
        "id": "wA7xIFuLoHZA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"ibm-granite/granite-3.2-2b-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_response(prompt, max_length=1024):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.replace(prompt, \"\").strip()\n",
        "    return response\n",
        "\n",
        "def disease_prediction(symptoms):\n",
        "    prompt = f\"Based on the following symptoms, provide possible medical conditions and general medication suggestions. Always emphasize the importance of consulting a doctor for proper diagnosis.\\n\\nSymptoms: {symptoms}\\n\\nPossible conditions and recommendations:\\n\\n**IMPORTANT: This is for informational purposes only. Please consult a healthcare professional for proper diagnosis and treatment.**\\n\\nAnalysis:\"\n",
        "    return generate_response(prompt, max_length=1200)\n",
        "\n",
        "def treatment_plan(condition, age, gender, medical_history):\n",
        "    prompt = f\"Generate personalized treatment suggestions for the following patient information. Include home remedies and general medication guidelines.\\n\\nMedical Condition: {condition}\\nAge: {age}\\nGender: {gender}\\nMedical History: {medical_history}\\n\\nPersonalized treatment plan including home remedies and medication guidelines:\\n\\n**IMPORTANT: This is for informational purposes only. Please consult a healthcare professional for proper treatment.**\\n\\nTreatment Plan:\"\n",
        "    return generate_response(prompt, max_length=1200)\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# Medical AI Assistant\")\n",
        "    gr.Markdown(\"**Disclaimer: This is for informational purposes only. Always consult healthcare professionals for medical advice.**\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Disease Prediction\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    symptoms_input = gr.Textbox(\n",
        "                        label=\"Enter Symptoms\",\n",
        "                        placeholder=\"e.g., fever, headache, cough, fatigue...\",\n",
        "                        lines=4\n",
        "                    )\n",
        "                    predict_btn = gr.Button(\"Analyze Symptoms\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    prediction_output = gr.Textbox(label=\"Possible Conditions & Recommendations\", lines=20)\n",
        "\n",
        "            predict_btn.click(disease_prediction, inputs=symptoms_input, outputs=prediction_output)\n",
        "\n",
        "        with gr.TabItem(\"Treatment Plans\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    condition_input = gr.Textbox(\n",
        "                        label=\"Medical Condition\",\n",
        "                        placeholder=\"e.g., diabetes, hypertension, migraine...\",\n",
        "                        lines=2\n",
        "                    )\n",
        "                    age_input = gr.Number(label=\"Age\", value=30)\n",
        "                    gender_input = gr.Dropdown(\n",
        "                        choices=[\"Male\", \"Female\", \"Other\"],\n",
        "                        label=\"Gender\",\n",
        "                        value=\"Male\"\n",
        "                    )\n",
        "                    history_input = gr.Textbox(\n",
        "                        label=\"Medical History\",\n",
        "                        placeholder=\"Previous conditions, allergies, medications or None\",\n",
        "                        lines=3\n",
        "                    )\n",
        "                    plan_btn = gr.Button(\"Generate Treatment Plan\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    plan_output = gr.Textbox(label=\"Personalized Treatment Plan\", lines=20)\n",
        "\n",
        "            plan_btn.click(treatment_plan, inputs=[condition_input, age_input, gender_input, history_input], outputs=plan_output)\n",
        "\n",
        "app.launch(share=True)"
      ],
      "metadata": {
        "id": "v8PGW6k0oKK4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}